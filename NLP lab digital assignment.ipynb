{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHIVA MUNNOORU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19MAI0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP  Digital assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY TOOL FOR NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One of the newest open-source Natural Language Processing with Python libraries on our list is SpaCy. It’s lightning-fast, easy to use, well-documented, and designed to support large volumes of data. Unlike NLTK or CoreNLP, which display a number of algorithms for each task, SpaCy keeps its menu short and serves up the best available option for each task at hand.This library is a great option if you want to prepare text for deep learning, and excels at extraction tasks. spaCy is designed specifically for production use and hepls you build  applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning. Unlike a platform, spaCy does not provide a software as a service, or a web application. It’s an open-source library designed to help you build NLP applications, not a consumable service. While spaCy can be used to power conversational applications, it’s not designed specifically for chat bots, and only provides the underlying text processing capabilities.  It’s built on the latest research, but it’s designed to get things done. This leads to fairly different design decisions than NLTK or CoreNLP, which were created as platforms for teaching and research. The main difference is that spaCy is integrated and opinionated. spaCy tries to avoid asking the user to choose between multiple algorithms that deliver equivalent functionality. Keeping the menu small lets spaCy deliver generally better performance and developer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are some features of spacy tool using following codes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little ADJ nsubjpass\n",
      "is AUX auxpass\n",
      "known VERB ROOT\n",
      "about ADP prep\n",
      "Shakespeare PROPN poss\n",
      "'s PART case\n",
      "activities NOUN pobj\n",
      "between ADP prep\n",
      "1585 NUM pobj\n",
      "and CCONJ cc\n",
      "1592 NUM conj\n",
      ". PUNCT punct\n",
      "Robert PROPN compound\n",
      "Greene PROPN poss\n",
      "'s PART case\n",
      "A PROPN compound\n",
      "Groatsworth PROPN nsubjpass\n",
      ", PUNCT punct\n",
      "the DET det\n",
      "London PROPN compound\n",
      "theaters NOUN nsubjpass\n",
      "were AUX auxpass\n",
      "often ADV advmod\n",
      "closed VERB ROOT\n",
      "between ADP prep\n",
      "June PROPN pobj\n",
      "1592 NUM nummod\n",
      "and CCONJ cc\n",
      "April PROPN conj\n",
      "1594 NUM nummod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "#spaCy provides a variety of linguistic annotations to give you insights into a text’s grammatical structure.\n",
    "#This includes the word types, like the parts of speech, and how the words are related to each other. \n",
    "#For example, if you’re analyzing text, it makes a huge difference whether a noun is the subject of a sentence, or the object – or whether “google” is used as a verb, or refers to the website or company in a specific context.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Little is known about Shakespeare's activities between 1585 and 1592. Robert Greene's A Groatsworth, the London theaters were often closed between June 1592 and April 1594.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little\n",
      "is\n",
      "known\n",
      "about\n",
      "Shakespeare\n",
      "'s\n",
      "activities\n",
      "between\n",
      "1585\n",
      "and\n",
      "1592\n",
      ".\n",
      "Robert\n",
      "Greene\n",
      "'s\n",
      "A\n",
      "Groatsworth\n",
      ",\n",
      "the\n",
      "London\n",
      "theaters\n",
      "were\n",
      "often\n",
      "closed\n",
      "between\n",
      "June\n",
      "1592\n",
      "and\n",
      "April\n",
      "1594\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on.\n",
    "#This is done by applying rules specific to each language.\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tags and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little little ADJ JJ nsubjpass Xxxxx True False\n",
      "is be AUX VBZ auxpass xx True True\n",
      "known know VERB VBN ROOT xxxx True False\n",
      "about about ADP IN prep xxxx True True\n",
      "Shakespeare Shakespeare PROPN NNP poss Xxxxx True False\n",
      "'s 's PART POS case 'x False True\n",
      "activities activity NOUN NNS pobj xxxx True False\n",
      "between between ADP IN prep xxxx True True\n",
      "1585 1585 NUM CD pobj dddd False False\n",
      "and and CCONJ CC cc xxx True True\n",
      "1592 1592 NUM CD conj dddd False False\n",
      ". . PUNCT . punct . False False\n",
      "Robert Robert PROPN NNP compound Xxxxx True False\n",
      "Greene Greene PROPN NNP poss Xxxxx True False\n",
      "'s 's PART POS case 'x False True\n",
      "A A PROPN NNP compound X True True\n",
      "Groatsworth Groatsworth PROPN NNP nsubjpass Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "the the DET DT det xxx True True\n",
      "London London PROPN NNP compound Xxxxx True False\n",
      "theaters theater NOUN NNS nsubjpass xxxx True False\n",
      "were be AUX VBD auxpass xxxx True True\n",
      "often often ADV RB advmod xxxx True True\n",
      "closed close VERB VBN ROOT xxxx True False\n",
      "between between ADP IN prep xxxx True True\n",
      "June June PROPN NNP pobj Xxxx True False\n",
      "1592 1592 NUM CD nummod dddd False False\n",
      "and and CCONJ CC cc xxx True True\n",
      "April April PROPN NNP conj Xxxxx True False\n",
      "1594 1594 NUM CD nummod dddd False False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "#After tokenization, spaCy can parse and tag a given doc.\n",
    "#This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare 22 33 PERSON\n",
      "between 1585 and 1592 47 68 DATE\n",
      "Robert Greene's 70 85 PERSON\n",
      "London 105 111 GPE\n",
      "June 1592 147 156 DATE\n",
      "April 1594 161 171 DATE\n"
     ]
    }
   ],
   "source": [
    "#A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title.\n",
    "#spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Little is known about Shakespeare's activities between 1585 and 1592. Robert Greene's A Groatsworth, the London theaters were often closed between June 1592 and April 1594.\")\n",
    "for ent in doc.ents:\n",
    " print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vectors and similarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6342116731412111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Similarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word.\n",
    "#spaCy is able to compare two objects, and make a prediction of how similar they are.\n",
    "#Predicting similarity is useful for building recommendation systems or flagging duplicates.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "search_doc = nlp(\"This was very strange argument between american and british person\")\n",
    "\n",
    "main_doc = nlp(\"He was from Japan, but a true English gentleman in my eyes, and another one of the reasons as to why I liked going to school.\")\n",
    "\n",
    "print(main_doc.similarity(search_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching text with token rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleIO Google I/O\n",
      "HAPPY 😀\n",
      "HAPPY 😀😀\n",
      "HAPPY 😀\n",
      "Sentiment 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "# here we match text with token rules, here we take some emojis to the text and we write a sample code we get the sentiment of a sentence.\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def set_sentiment(matcher, doc, i, matches):\n",
    "    doc.sentiment += 0.1\n",
    "\n",
    "pattern1 = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "pattern2 = [[{\"ORTH\": emoji, \"OP\": \"+\"}] for emoji in [\"😀\", \"😂\", \"🤣\", \"😍\"]]\n",
    "matcher.add(\"GoogleIO\", None, pattern1)  # Match \"Google I/O\" or \"Google i/o\"\n",
    "matcher.add(\"HAPPY\", set_sentiment, *pattern2)  # Match one or more happy emoji\n",
    "\n",
    "doc = nlp(\"A text about Google I/O 😀😀\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)\n",
    "print(\"Sentiment\", doc.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop words are the most common words in a language. In the English language, some examples of stop words are the, are, but, and they.\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "much\n",
      "however\n",
      "go\n",
      "through\n",
      "nor\n",
      "others\n",
      "take\n",
      "along\n",
      "everywhere\n",
      "various\n",
      "other\n",
      "give\n",
      "something\n",
      "yours\n",
      "across\n",
      "throughout\n",
      "both\n",
      "formerly\n",
      "side\n",
      "ourselves\n"
     ]
    }
   ],
   "source": [
    "# stop words are removed because they aren’t significant and distort the word frequency analysis.\n",
    "for stop_word in list(spacy_stopwords)[:20]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using hash values for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12960314961752105148 cricket\n",
      "12960314961752105148 12960314961752105148\n",
      "cricket cricket\n",
      "2318904499095803474 deer\n"
     ]
    }
   ],
   "source": [
    "#To save memory, spaCy also encodes all strings to hash values \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love cricket\")\n",
    "coffee_hash = nlp.vocab.strings[\"cricket\"] # 3197928453018144401\n",
    "coffee_text = nlp.vocab.strings[coffee_hash] # 'coffee'\n",
    "print(coffee_hash, coffee_text)\n",
    "print(doc[2].orth, coffee_hash) # 3197928453018144401\n",
    "print(doc[2].text, coffee_text) # 'coffee'\n",
    "deer_hash = doc.vocab.strings.add(\"deer\") # 3073001599257881079\n",
    "deer_text = doc.vocab.strings[deer_hash] # 'beer'\n",
    "print(deer_hash, deer_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spacy.io/usage/spacy-101\n",
    "#https://realpython.com/natural-language-processing-spacy-python/#:~:text=spaCy%20is%20a%20free%2C%20open,concise%20and%20user%2Dfriendly%20API.\n",
    "#https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
